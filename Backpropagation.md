Backpropagation is **a [[Machine Learning]] technique essential to the optimization of artificial neural networks**. It facilitates the use of gradient descent algorithms to update network weights, which is how the deep learning models driving modern artificial intelligence (AI) “learn.”

1. Calculates the total error 𝐿(𝑤) and then the contribution to the error at each step going backwards with a variety of error calculation methods: mean Square Error (MSE), sum of squared errors (SSE), cross Entropy (softmax).  
2. Optimize the weights so that the neural network can learn how to correctly map arbitrary inputs to outputs.
