Stochastic gradient descent, often abbreviated SGD, is a variant of [[Gradient Descent]] calculates the error and updates the model for each example in the training dataset.
# Advantage  
• Adding more noise to the learning process than mini- batch helps to improve  
generalization error.
• Faster updates can result in faster learning on some problems.  
• Resource efficiency.
# Disadvantage  
• Frequent updates is more computationally expensive than the other gradient descent configurations.  
• The noisy learning process causes the model harder to converge.