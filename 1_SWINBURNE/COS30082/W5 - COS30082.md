[[COS30082]]
# Convolutional Neural Network
See absolutely everything in 
[[Convolutional Neural Network]]
# Optimization Algorithms
Recall the traditional optimization algorithms we have discussed previously:
[[Batch Gradient Descent]]
[[Stochastic Gradient Descent]]
[[Mini-batch Gradient Descent]]

The update equation for normal stochastic gradient descent:
$$\Large
w_t \leftarrow w_{t-1} \leftarrow \alpha \begin{bmatrix} \frac{\delta J}{\delta w} \end{bmatrix}_{w_{t-1}}
$$
Using the similar equation, ùë§ is often chosen to be updated based on mini-batch GD.

See all in [[Optimization Algorithm]]

# State of the art CNNs
## LeNet-5
Proposed by Yann LeCun, Leon Bottou, Yosuha Bengio and Patrick Haffner  
in 1990‚Äôs. 
Designed for handwritten and machine-printed character recognition.

![[lenet5.png]]

## AlexNet
The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).

![[alexnet.png]]

**Special Features:**
- ReLU nonlinearity instead of tanh and sigmoid ‚Äì improve training time  
- Multiple GPU ‚Äì allow bigger model, improve training time  
- Overlapping pooling ‚Äì improve accuracy  
- Data augmentation ‚Äì reduce overfitting  
- [[Dropout]] ‚Äì a regularization approach to reduce overfitting
## VGG
VGG is an improvement over AlexNet. Its main contribution has been to show that network depth is an essential element for good performance.

**Special Features**:
- It replaces the large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) in AlexNet with multiple 3X3 kernel-sized filters one after another.  
- Blocks with the same filter size are applied multiple times to extract more complex and representative features. This concept of blocks/modules became a common theme in networks after the VGG.  
- Limitation: It has a lot of parameters (140M). Most of these parameters are in the first fully connected layer.

All configurations follow the generic design present in architecture and differ only in the depth.  
	E.g., 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers).  
	Their final best network is VGG-16 (13 conv. and 3 FC layers).
![[VGG config.png]]
## GoogleNet
Googlenet main contribution was the development of an [[Inception Layer]] that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M).
## ResNet
Problem of deeper networks  
	Vanishing gradient ‚Äì Neglecting the earlier layers.  
	Curse of dimensionality ‚Äì Causes degradation problem. Shallower networks sometimes learn better than their deeper counterparts.  

Residual Network was developed by Kaiming He et al. to solve the above problems.

ResNet uses skip connections, also known as residual connections, which allow gradients to flow through a network directly, bypassing multiple layers without attenuation during backpropagation. These connections add the output from an earlier layer to a later layer, effectively creating shorter paths for the gradient signal and thus mitigating the vanishing gradient problem.

# Batch Normalization
[[Batch Normalization]]
# Practical Methodology
[[Practical Design Process for the Application of Deep Learning Techniques]]

