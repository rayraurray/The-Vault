Gradient Vanishing occurs when gradients become increasingly small  
during backpropagation, causing weights in earlier layers to barely  
update.