[[COS30082]]
# Support Vector Machine
[[Support Vector Machine (SVM)]]
# SVM vs [[Logistic Regression]]

| SVM                                                                                                                                       | Logistic Regression                                                                                         |
| ----------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| It is able to find the maximum margin (distance between the  <br>decision boundary and the support vectors) that separates the 2 classes. | It can have different decision boundaries with different parameters 𝜃s that are near the optimal solution. |

![[SVM vs Logistic Regression.png]]

When to use Logistic Regression or SVM => Based on the number of training samples ($n$) and the number of features ($m$).
- If $m$ is large and n is small where $m >> n$: Linear SVM or Logistic Regression might be a choice.
- If $m$  is small and n is intermediate where $n > m$: SVM with Gaussian kernel might be a choice.
- If $m$  is small and n is large where $n >> m$ : Linear SVM or Logistic Regression might be a choice.
# SVM vs [[Artificial Neural Network (ANN)]]
SVMs transform the classification task into a convex optimization problem that guides towards the global optimum, whereas ANN can suffer from multiple local minima.  
SVMs are inherently binary classifiers, which means they find a hyperplane separating between two classes, whereas ANN can handle many classes by design.  
ANN generally performs better in a very large dataset.  
However, it is not possible to conclude which one performs the best because it depends on the problem and the dataset

# Why Non-Linear Hypothesis?
Imagine, the size of the features is the image size: $n$ = 60 x 60 pixels  
Hence, we have 3600 number of features per image (10800 for RGB images).  
If we want to achieve non linear hypothesis and will to feed in quadratic features ($x_2$), then
$n$ ≈ 58 million.  
Features that are too large are not a good way of representation and lead to high computational cost.  
Neural network provides a better alternative to learn non-linear model.
# Artificial Neural Network (ANN)
[[Artificial Neural Network (ANN)]]
# Neuron Model for Logistic Regression
In binary classification, to ensure predicted value $ℎ_𝜃 (x)$ lies between 0 and 1:
$$\Large 1 ≤ ℎ_𝜃 (𝑥) ≤0$$
$h_𝜃(𝑥)$ is represented as → $\large ℎ_𝜃(𝑥) = 𝑔(𝑧)$
where 
$\large z = 𝜃^T x$
$\large g(z) = \frac{1}{1 + e^{-z}}$
![[Neuron Model for LogR.png]]

# Neuron Model for ANN
See Neuron Model in [[Artificial Neural Network (ANN)]]

# Feature Learning Capability
Logistic regression can be considered as a neural network without a hidden layer. Its mapping function connects the input $x$ to the output $y$ by a sigmoid function of a linear combination of features.  

A neural network is more complex than logistic regression. Normally, it has at least one hidden layer, which consists of the new features ($a^{(2)}$) that are learned as a function of $f(𝜃^{(1)} , 𝑥)$.  

With such flexibility in feature learning, neural networks can learn more complex features and output a more complex non-linear hypothesis.
# Multiple Output Neurons
![[Multiple Output Neurons.png]]

# Logistic Regression vs NN

| Neural Network                                                                                     | Logistic Regression                                                                   |
| -------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |
| Neural networks are much better for a  <br>**complex nonlinear hypothesis**.                       | LR is useful for dataset where the classes  <br>are more or less “linearly separable” |
| ![[Complex Nonlinear Hypothesis.png]]                                                              | For “relatively” very small dataset.                                                  |
| A neural network is in general **not convex.**  <br>It can suffer from multiple local  <br>minima. | It is a great, robust model for simple  <br>classification tasks.                     |
| ![[Not-convex.png]]                                                                                | ![[Convex.png]]                                                                       |

LR cost function (or max-entropy) is convex, and thus we are guaranteed to find the global cost minimum.